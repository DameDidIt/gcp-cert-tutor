{
  "questions": [
    {
      "domain_id": 1,
      "subtopic": "Setting up cloud projects and accounts",
      "stem": "Your company wants to enforce a policy that VMs can only be created in us-central1 and europe-west1 across all projects. What should you use?",
      "choice_a": "IAM roles restricting compute.instances.create",
      "choice_b": "Organization Policy with a resource location constraint",
      "choice_c": "VPC firewall rules blocking other regions",
      "choice_d": "Custom roles with region-specific permissions",
      "correct_answer": "b",
      "explanation": "Organization Policies with the constraints/gcp.resourceLocations constraint allow you to restrict which regions resources can be created in across the entire organization or specific folders/projects."
    },
    {
      "domain_id": 1,
      "subtopic": "Setting up cloud projects and accounts",
      "stem": "What is the correct hierarchy of GCP resources from broadest to most specific?",
      "choice_a": "Project → Folder → Organization → Resource",
      "choice_b": "Organization → Project → Folder → Resource",
      "choice_c": "Organization → Folder → Project → Resource",
      "choice_d": "Folder → Organization → Project → Resource",
      "correct_answer": "c",
      "explanation": "The GCP resource hierarchy is Organization → Folders (optional, nestable) → Projects → Resources. IAM policies and organization policies are inherited downward through this hierarchy."
    },
    {
      "domain_id": 1,
      "subtopic": "Setting up cloud projects and accounts",
      "stem": "A developer needs to use the Cloud Translation API in their project but gets a 'PERMISSION_DENIED' error. The developer has the Editor role. What is the most likely cause?",
      "choice_a": "The developer needs the Owner role",
      "choice_b": "The Cloud Translation API has not been enabled in the project",
      "choice_c": "The developer's account has been suspended",
      "choice_d": "The project has exceeded its quota",
      "correct_answer": "b",
      "explanation": "APIs must be explicitly enabled in a project before they can be used. Even with the Editor role, if the API is not enabled, calls will fail with PERMISSION_DENIED. Enable it with gcloud services enable translate.googleapis.com."
    },
    {
      "domain_id": 1,
      "subtopic": "Setting up cloud projects and accounts",
      "stem": "You need to create an identity for a contractor who does not have a Google account. What should you use?",
      "choice_a": "Create a service account for the contractor",
      "choice_b": "Use Cloud Identity to create a managed user account",
      "choice_c": "Share your credentials with the contractor",
      "choice_d": "Create a custom IAM role for the contractor",
      "correct_answer": "b",
      "explanation": "Cloud Identity allows you to create managed user accounts for people who don't have Google accounts. This provides an identity that can be used with IAM policies. Service accounts are for applications, not people."
    },
    {
      "domain_id": 1,
      "subtopic": "Setting up cloud projects and accounts",
      "stem": "Which gcloud command correctly sets the default region and zone for your CLI configuration?",
      "choice_a": "gcloud config set compute/region us-central1 && gcloud config set compute/zone us-central1-a",
      "choice_b": "gcloud config set region us-central1 && gcloud config set zone us-central1-a",
      "choice_c": "gcloud defaults set region=us-central1 zone=us-central1-a",
      "choice_d": "gcloud compute set-defaults --region=us-central1 --zone=us-central1-a",
      "correct_answer": "a",
      "explanation": "The correct properties are compute/region and compute/zone under gcloud config set. These become the defaults for Compute Engine commands when --region or --zone flags are not specified."
    },
    {
      "domain_id": 1,
      "subtopic": "Managing billing configuration",
      "stem": "You want to automatically receive an email when your project spending exceeds 80% of your monthly budget. What should you configure?",
      "choice_a": "A billing export to BigQuery with a scheduled query",
      "choice_b": "A budget with an alert threshold at 80%",
      "choice_c": "A Cloud Monitoring alerting policy on billing metrics",
      "choice_d": "A Cloud Function that checks billing data hourly",
      "correct_answer": "b",
      "explanation": "Budgets in the Billing section allow you to set alert thresholds (e.g., 50%, 80%, 100%). When spending reaches these thresholds, notification emails are sent to billing admins and configured recipients."
    },
    {
      "domain_id": 1,
      "subtopic": "Managing billing configuration",
      "stem": "Your organization needs to analyze detailed cost data across all projects. What is the recommended approach?",
      "choice_a": "Export billing data to Cloud Storage as CSV files",
      "choice_b": "Use the Billing Reports page in the Console",
      "choice_c": "Export billing data to BigQuery",
      "choice_d": "Use the Pricing Calculator for each service",
      "correct_answer": "c",
      "explanation": "BigQuery billing export provides detailed, queryable usage and cost data for all projects. It is the recommended approach for detailed cost analysis, custom reporting, and programmatic cost management."
    },
    {
      "domain_id": 1,
      "subtopic": "Managing billing configuration",
      "stem": "A project team wants to programmatically shut down resources when spending exceeds the budget. What is the recommended architecture?",
      "choice_a": "Create a Cloud Scheduler job that checks billing daily",
      "choice_b": "Configure a budget with Pub/Sub notification and a Cloud Function to disable billing",
      "choice_c": "Set up a VM that polls the Billing API every minute",
      "choice_d": "Use Organization Policy to set spending limits",
      "correct_answer": "b",
      "explanation": "The recommended pattern is: Budget alert → Pub/Sub topic → Cloud Function. The function can programmatically disable the billing account or shut down resources when the budget threshold is exceeded."
    },
    {
      "domain_id": 1,
      "subtopic": "Managing billing configuration",
      "stem": "You have 10 projects that should all be charged to the same billing account. One project currently has no billing. What role do you need on the billing account to link the project?",
      "choice_a": "roles/billing.admin",
      "choice_b": "roles/billing.viewer",
      "choice_c": "roles/billing.user",
      "choice_d": "roles/billing.creator",
      "correct_answer": "c",
      "explanation": "roles/billing.user on the billing account allows you to link projects to that billing account. You also need appropriate permissions on the project (e.g., roles/owner or roles/resourcemanager.projectCreator)."
    },
    {
      "domain_id": 1,
      "subtopic": "Managing billing configuration",
      "stem": "What happens to running Compute Engine VMs when the billing account linked to their project is disabled?",
      "choice_a": "VMs continue running but cannot be modified",
      "choice_b": "VMs are immediately deleted",
      "choice_c": "VMs are stopped and eventually deleted if billing is not re-enabled",
      "choice_d": "VMs continue running for 30 days grace period",
      "correct_answer": "c",
      "explanation": "When billing is disabled, VMs are stopped (not immediately deleted). There is a grace period during which you can re-enable billing to recover resources. After the grace period, resources may be permanently deleted."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring compute resources",
      "stem": "Your application needs to run batch processing jobs that can tolerate interruptions and you want to minimize costs. Which compute option should you use?",
      "choice_a": "Standard N2 VMs with sustained use discounts",
      "choice_b": "Spot VMs",
      "choice_c": "Cloud Run jobs",
      "choice_d": "Committed use discounts on E2 VMs",
      "correct_answer": "b",
      "explanation": "Spot VMs offer 60-91% discounts compared to standard VMs. They can be preempted at any time, making them ideal for fault-tolerant batch processing workloads that can handle interruptions."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring compute resources",
      "stem": "You need to run a containerized web application that receives variable traffic and should scale to zero when idle. Which service is most appropriate?",
      "choice_a": "Compute Engine with a managed instance group",
      "choice_b": "GKE Autopilot",
      "choice_c": "Cloud Run",
      "choice_d": "Cloud Functions",
      "correct_answer": "c",
      "explanation": "Cloud Run automatically scales containers from 0 to N based on traffic. It is ideal for containerized web apps with variable traffic since you only pay when requests are being processed."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring compute resources",
      "stem": "Your workload requires 6 vCPUs and 20 GB of RAM. No predefined machine type matches these specs. What should you use?",
      "choice_a": "The next larger predefined machine type",
      "choice_b": "A custom machine type with 6 vCPUs and 20 GB RAM",
      "choice_c": "Two smaller VMs with a load balancer",
      "choice_d": "An E2-medium with extended memory",
      "correct_answer": "b",
      "explanation": "Custom machine types allow you to specify exact vCPU and memory combinations, avoiding overpaying for a larger predefined type. Create with --custom-cpu=6 --custom-memory=20GB."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring compute resources",
      "stem": "You have a predictable workload that will run continuously for the next 3 years. How can you maximize cost savings?",
      "choice_a": "Use Spot VMs",
      "choice_b": "Use sustained use discounts",
      "choice_c": "Purchase 3-year committed use discounts",
      "choice_d": "Use E2 machine types",
      "correct_answer": "c",
      "explanation": "3-year committed use discounts provide up to 55% savings for a predictable, continuous workload. This is the maximum discount for guaranteed, long-running workloads."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring compute resources",
      "stem": "Which GKE mode should you choose if you want Google to fully manage the node infrastructure and enforce security best practices?",
      "choice_a": "GKE Standard with auto-provisioning",
      "choice_b": "GKE Autopilot",
      "choice_c": "GKE Enterprise",
      "choice_d": "GKE Standard with cluster autoscaler",
      "correct_answer": "b",
      "explanation": "GKE Autopilot fully manages the node infrastructure. Google handles node provisioning, scaling, security hardening, and OS updates. You only define your pod specifications."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring data storage options",
      "stem": "You need a globally distributed relational database with strong consistency and horizontal scalability for a financial application. Which service should you use?",
      "choice_a": "Cloud SQL with read replicas",
      "choice_b": "Cloud Spanner",
      "choice_c": "Firestore in Native mode",
      "choice_d": "BigQuery",
      "correct_answer": "b",
      "explanation": "Cloud Spanner provides global distribution, horizontal scalability, and strong consistency for relational data. It is designed for mission-critical, globally distributed applications like financial systems."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring data storage options",
      "stem": "Your application stores user profile photos and videos. Which storage service is most appropriate?",
      "choice_a": "Cloud SQL",
      "choice_b": "Firestore",
      "choice_c": "Cloud Storage",
      "choice_d": "Bigtable",
      "correct_answer": "c",
      "explanation": "Cloud Storage is designed for unstructured data like images, videos, and files. It offers multiple storage classes, global edge caching with Cloud CDN, and virtually unlimited capacity."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring data storage options",
      "stem": "You have archival data that is accessed less than once a year. Which Cloud Storage class minimizes cost?",
      "choice_a": "Standard",
      "choice_b": "Nearline",
      "choice_c": "Coldline",
      "choice_d": "Archive",
      "correct_answer": "d",
      "explanation": "Archive storage class has the lowest storage cost and is designed for data accessed less than once a year. It has a 365-day minimum storage duration and higher retrieval costs."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring data storage options",
      "stem": "You need a NoSQL database with real-time synchronization for a mobile app. Which service should you use?",
      "choice_a": "Cloud SQL",
      "choice_b": "Firestore in Native mode",
      "choice_c": "Firestore in Datastore mode",
      "choice_d": "Bigtable",
      "correct_answer": "b",
      "explanation": "Firestore in Native mode provides real-time sync, offline support, and is optimized for mobile and web applications. Datastore mode does not support real-time listeners."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring data storage options",
      "stem": "Your analytics team needs to run complex SQL queries on petabytes of data. Which service is best suited?",
      "choice_a": "Cloud SQL",
      "choice_b": "Cloud Spanner",
      "choice_c": "BigQuery",
      "choice_d": "Bigtable",
      "correct_answer": "c",
      "explanation": "BigQuery is a serverless data warehouse designed for OLAP workloads. It can run complex SQL queries on petabytes of data quickly and cost-effectively. It is the primary analytics service in GCP."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring network resources",
      "stem": "You need to serve a global web application with the lowest possible latency. Which load balancer type should you use?",
      "choice_a": "Internal TCP/UDP Load Balancer",
      "choice_b": "External HTTP(S) Load Balancer (global)",
      "choice_c": "External Network Load Balancer (regional)",
      "choice_d": "Internal HTTP(S) Load Balancer",
      "correct_answer": "b",
      "explanation": "The global External HTTP(S) Load Balancer provides a single anycast IP, routes traffic to the nearest backend globally, integrates with Cloud CDN, and supports SSL termination for lowest latency."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring network resources",
      "stem": "Your company needs high-bandwidth, low-latency private connectivity between your data center and GCP. Which service should you use?",
      "choice_a": "Cloud VPN",
      "choice_b": "VPC Network Peering",
      "choice_c": "Dedicated Interconnect",
      "choice_d": "Cloud CDN",
      "correct_answer": "c",
      "explanation": "Dedicated Interconnect provides high-bandwidth (10/100 Gbps), low-latency private connectivity between on-premises and GCP. It does not traverse the public internet, providing consistent performance."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring network resources",
      "stem": "You want to reduce network costs for regional traffic that does not require Google's global backbone. Which Network Service Tier should you choose?",
      "choice_a": "Premium Tier",
      "choice_b": "Standard Tier",
      "choice_c": "Basic Tier",
      "choice_d": "Enterprise Tier",
      "correct_answer": "b",
      "explanation": "Standard Tier routes traffic over the public internet, reducing costs for regional workloads that do not need the performance benefits of Google's global network. It does not support global load balancing."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring network resources",
      "stem": "Two VPC networks in different projects need to communicate using internal IP addresses. Which option allows this without a VPN?",
      "choice_a": "Cloud Router",
      "choice_b": "Cloud NAT",
      "choice_c": "VPC Network Peering",
      "choice_d": "Cloud Armor",
      "correct_answer": "c",
      "explanation": "VPC Network Peering connects two VPC networks allowing communication via internal IPs. Traffic stays on Google's network without needing a VPN. It works across projects and organizations."
    },
    {
      "domain_id": 2,
      "subtopic": "Planning and configuring network resources",
      "stem": "Your organization has multiple projects and wants centralized network administration with shared subnets. What should you use?",
      "choice_a": "VPC Network Peering between all projects",
      "choice_b": "Shared VPC with a host project",
      "choice_c": "Cloud VPN tunnels between projects",
      "choice_d": "Cloud Interconnect",
      "correct_answer": "b",
      "explanation": "Shared VPC allows a host project to share its VPC subnets with service projects. This centralizes network administration while allowing individual project teams to manage their own resources."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Compute Engine resources",
      "stem": "You need to create a managed instance group that automatically scales based on CPU usage. Which sequence of steps is correct?",
      "choice_a": "Create instance template → Create MIG → Configure autoscaler",
      "choice_b": "Create MIG → Create instance template → Configure autoscaler",
      "choice_c": "Create autoscaler → Create instance template → Create MIG",
      "choice_d": "Create instance → Create snapshot → Create MIG",
      "correct_answer": "a",
      "explanation": "You must first create an instance template that defines the VM configuration, then create a managed instance group from that template, then configure the autoscaler with target CPU utilization."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Compute Engine resources",
      "stem": "You want to ensure that SSH access to your VMs is managed through IAM roles instead of SSH keys stored in metadata. What should you enable?",
      "choice_a": "IAP TCP forwarding",
      "choice_b": "OS Login",
      "choice_c": "Serial port access",
      "choice_d": "Service account key authentication",
      "correct_answer": "b",
      "explanation": "OS Login links Linux user accounts to Google Cloud identities and manages SSH access through IAM roles (roles/compute.osLogin or roles/compute.osAdminLogin) instead of metadata-based SSH keys."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Compute Engine resources",
      "stem": "You need a group of identical VMs that automatically replaces unhealthy instances. What should you create?",
      "choice_a": "An unmanaged instance group with a health check",
      "choice_b": "A managed instance group with auto-healing",
      "choice_c": "Multiple individual VMs with a startup script",
      "choice_d": "A managed instance group with no health check",
      "correct_answer": "b",
      "explanation": "A managed instance group (MIG) with auto-healing uses a health check to detect unhealthy instances and automatically recreates them. Unmanaged instance groups do not support auto-healing."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Compute Engine resources",
      "stem": "You want higher availability for your managed instance group. What should you use?",
      "choice_a": "A zonal MIG with more instances",
      "choice_b": "A regional MIG spread across multiple zones",
      "choice_c": "Multiple zonal MIGs in different zones",
      "choice_d": "An unmanaged instance group",
      "correct_answer": "b",
      "explanation": "A regional MIG distributes instances across multiple zones in a region, providing higher availability. If one zone fails, instances in other zones continue serving traffic."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Compute Engine resources",
      "stem": "What is the correct gcloud command to create a VM with a Debian image and allow HTTP traffic?",
      "choice_a": "gcloud compute instances create my-vm --image-family=debian-11 --image-project=debian-cloud --tags=http-server",
      "choice_b": "gcloud compute instances create my-vm --os=debian-11 --firewall=allow-http",
      "choice_c": "gcloud compute vms create my-vm --image=debian-11 --allow-http",
      "choice_d": "gcloud instances create my-vm --image-family=debian-11 --tags=http-server",
      "correct_answer": "a",
      "explanation": "The correct syntax uses 'gcloud compute instances create' with --image-family and --image-project to specify the OS, and --tags=http-server to apply the network tag that matches the default allow-http firewall rule."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying GKE resources",
      "stem": "You need to deploy a containerized app to GKE. The cluster already exists. What is the correct order of commands?",
      "choice_a": "kubectl apply → gcloud container clusters get-credentials → kubectl expose",
      "choice_b": "gcloud container clusters get-credentials → kubectl create deployment → kubectl expose",
      "choice_c": "docker push → kubectl create deployment → kubectl expose",
      "choice_d": "gcloud container clusters get-credentials → docker push → kubectl apply",
      "correct_answer": "b",
      "explanation": "First get cluster credentials with gcloud container clusters get-credentials to configure kubectl. Then create the deployment with kubectl, then expose it as a service."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying GKE resources",
      "stem": "You want a GKE cluster where Google manages the nodes and you only define pod-level resources. What should you create?",
      "choice_a": "GKE Standard with node auto-provisioning",
      "choice_b": "GKE Autopilot cluster",
      "choice_c": "GKE Standard with cluster autoscaler",
      "choice_d": "GKE Enterprise cluster",
      "correct_answer": "b",
      "explanation": "GKE Autopilot manages all node infrastructure. You define pods and Autopilot handles node provisioning, scaling, and security. You are billed per pod resource request, not per node."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying GKE resources",
      "stem": "Your security team requires that GKE cluster nodes have no public IP addresses. What cluster configuration should you use?",
      "choice_a": "Standard cluster with Cloud NAT",
      "choice_b": "Private cluster",
      "choice_c": "Cluster with Network Policy enabled",
      "choice_d": "Cluster with Binary Authorization",
      "correct_answer": "b",
      "explanation": "A private GKE cluster assigns only internal IP addresses to nodes, isolating them from the public internet. The control plane can be accessed via authorized networks or a private endpoint."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying GKE resources",
      "stem": "You need to create a regional GKE cluster for high availability. Which command is correct?",
      "choice_a": "gcloud container clusters create my-cluster --zone=us-central1-a --num-nodes=3",
      "choice_b": "gcloud container clusters create my-cluster --region=us-central1 --num-nodes=1",
      "choice_c": "gcloud container clusters create my-cluster --region=us-central1 --replicas=3",
      "choice_d": "gcloud container clusters create my-cluster --multi-zone --num-nodes=3",
      "correct_answer": "b",
      "explanation": "Using --region instead of --zone creates a regional cluster. The control plane and nodes are spread across 3 zones. --num-nodes=1 means 1 node per zone (3 total)."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying GKE resources",
      "stem": "You need to store Docker images for your GKE workloads. Which service should you use?",
      "choice_a": "Cloud Storage",
      "choice_b": "Container Registry",
      "choice_c": "Artifact Registry",
      "choice_d": "Cloud Build",
      "correct_answer": "c",
      "explanation": "Artifact Registry is the recommended service for storing Docker images (and other artifact types). Container Registry is deprecated. Artifact Registry provides fine-grained IAM and supports multiple formats."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Cloud Run and Cloud Functions",
      "stem": "You want to deploy a Python web application to Cloud Run. The application listens on port 5000. What flag do you need?",
      "choice_a": "--port=5000",
      "choice_b": "--container-port=5000",
      "choice_c": "--listen-port=5000",
      "choice_d": "No flag needed; Cloud Run auto-detects the port",
      "correct_answer": "a",
      "explanation": "Use --port=5000 to tell Cloud Run which port to send requests to. The default is 8080. The container must listen on 0.0.0.0 (not localhost) on the specified port."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Cloud Run and Cloud Functions",
      "stem": "You need an event-driven function that processes messages from a Pub/Sub topic. Which is the simplest approach?",
      "choice_a": "Deploy a Cloud Run service with a Pub/Sub push subscription",
      "choice_b": "Deploy a Cloud Function with --trigger-topic",
      "choice_c": "Deploy a Compute Engine VM that polls the Pub/Sub topic",
      "choice_d": "Deploy a GKE pod with a Pub/Sub pull subscription",
      "correct_answer": "b",
      "explanation": "Cloud Functions with --trigger-topic provides the simplest event-driven integration with Pub/Sub. The function is automatically triggered when a message is published to the topic."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Cloud Run and Cloud Functions",
      "stem": "Your Cloud Run service experiences cold starts that impact user experience. How do you minimize this?",
      "choice_a": "Increase the CPU allocation",
      "choice_b": "Set --min-instances to a value greater than 0",
      "choice_c": "Enable Cloud CDN",
      "choice_d": "Increase the --concurrency setting",
      "correct_answer": "b",
      "explanation": "Setting --min-instances > 0 keeps container instances warm, eliminating cold starts. The minimum instances are always running and ready to handle requests, though this increases cost."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Cloud Run and Cloud Functions",
      "stem": "You want to route events from Cloud Storage object creation to a Cloud Run service. What service should you use?",
      "choice_a": "Cloud Pub/Sub directly",
      "choice_b": "Eventarc",
      "choice_c": "Cloud Scheduler",
      "choice_d": "Cloud Tasks",
      "correct_answer": "b",
      "explanation": "Eventarc provides a unified eventing experience, routing events from Google Cloud sources (like Cloud Storage) to Cloud Run, GKE, or Workflows. It simplifies event-driven architectures."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying Cloud Run and Cloud Functions",
      "stem": "What is a key advantage of Cloud Functions 2nd gen over 1st gen?",
      "choice_a": "Lower cost per invocation",
      "choice_b": "Support for more programming languages",
      "choice_c": "Longer timeout (up to 60 minutes) and concurrency support",
      "choice_d": "Built-in database connections",
      "correct_answer": "c",
      "explanation": "Cloud Functions 2nd gen (built on Cloud Run) supports timeouts up to 60 minutes (vs 9 min for 1st gen), concurrency (multiple requests per instance), larger instances, and traffic splitting."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying data solutions",
      "stem": "You need to deploy a MySQL database with automatic backups and high availability. Which service should you use?",
      "choice_a": "Cloud SQL with HA configuration",
      "choice_b": "Cloud Spanner",
      "choice_c": "Compute Engine with MySQL installed",
      "choice_d": "Firestore in Datastore mode",
      "correct_answer": "a",
      "explanation": "Cloud SQL provides managed MySQL with automatic backups, point-in-time recovery, and high availability (regional instances with automatic failover). It handles patching and maintenance automatically."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying data solutions",
      "stem": "You need to create a Pub/Sub subscription that writes messages directly to BigQuery. Which subscription type should you use?",
      "choice_a": "Pull subscription",
      "choice_b": "Push subscription",
      "choice_c": "BigQuery subscription",
      "choice_d": "Cloud Storage subscription",
      "correct_answer": "c",
      "explanation": "Pub/Sub BigQuery subscription writes messages directly to a BigQuery table without any intermediate processing. This is the simplest way to stream Pub/Sub data into BigQuery."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying data solutions",
      "stem": "You are importing a large SQL dump file into Cloud SQL. Where should the dump file be stored?",
      "choice_a": "On the local machine running gcloud",
      "choice_b": "In a Cloud Storage bucket",
      "choice_c": "On a Compute Engine VM",
      "choice_d": "In BigQuery",
      "correct_answer": "b",
      "explanation": "Cloud SQL imports require the source file to be in Cloud Storage. Use gcloud sql import sql INSTANCE gs://BUCKET/FILE.sql --database=DB. The Cloud SQL service account needs read access to the bucket."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying data solutions",
      "stem": "You need a fully managed, PostgreSQL-compatible database with high performance for both transactional and analytical queries. Which service should you choose?",
      "choice_a": "Cloud SQL for PostgreSQL",
      "choice_b": "Cloud Spanner",
      "choice_c": "AlloyDB",
      "choice_d": "BigQuery",
      "correct_answer": "c",
      "explanation": "AlloyDB is a fully managed PostgreSQL-compatible database optimized for demanding workloads. It offers up to 4x faster transactions and 100x faster analytical queries compared to standard PostgreSQL."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying data solutions",
      "stem": "You need to set up a real-time data processing pipeline that reads from Pub/Sub and writes to BigQuery. Which service should you use?",
      "choice_a": "Cloud Composer",
      "choice_b": "Dataflow",
      "choice_c": "Dataproc",
      "choice_d": "Cloud Scheduler",
      "correct_answer": "b",
      "explanation": "Dataflow is a fully managed stream and batch processing service based on Apache Beam. It is ideal for real-time ETL pipelines that read from Pub/Sub and write to BigQuery or other sinks."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying networking resources",
      "stem": "You are creating a new VPC for a project that requires full control over subnet IP ranges. What subnet mode should you use?",
      "choice_a": "Auto mode",
      "choice_b": "Custom mode",
      "choice_c": "Legacy mode",
      "choice_d": "Default mode",
      "correct_answer": "b",
      "explanation": "Custom mode VPCs let you manually create subnets with specific IP ranges. Auto mode creates subnets automatically in all regions with predefined ranges, which may conflict with other networks."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying networking resources",
      "stem": "You need to allow HTTP traffic to VMs tagged as 'web-server'. Which gcloud command is correct?",
      "choice_a": "gcloud compute firewall-rules create allow-http --network=my-vpc --allow=tcp:80 --target-tags=web-server",
      "choice_b": "gcloud compute firewall-rules create allow-http --network=my-vpc --allow=http --target-tags=web-server",
      "choice_c": "gcloud compute security-rules create allow-http --allow=tcp:80 --targets=web-server",
      "choice_d": "gcloud compute firewall-rules create allow-http --allow=tcp:80 --source-tags=web-server",
      "correct_answer": "a",
      "explanation": "The correct syntax uses --allow=tcp:80 (not 'http') and --target-tags to specify which VMs the rule applies to. --source-tags would filter by source, not destination."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying networking resources",
      "stem": "You need to connect two VPC networks in different projects so they can communicate via internal IPs. You want a non-transitive connection. What should you use?",
      "choice_a": "Cloud VPN",
      "choice_b": "VPC Network Peering",
      "choice_c": "Shared VPC",
      "choice_d": "Cloud Interconnect",
      "correct_answer": "b",
      "explanation": "VPC Network Peering provides non-transitive connectivity between two VPCs using internal IPs. Traffic stays on Google's network. It must be configured on both sides."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying networking resources",
      "stem": "Your organization needs a VPN with 99.99% availability SLA between on-premises and GCP. Which option should you choose?",
      "choice_a": "Classic VPN",
      "choice_b": "HA VPN",
      "choice_c": "Cloud Interconnect",
      "choice_d": "Cloud Router",
      "correct_answer": "b",
      "explanation": "HA VPN provides 99.99% SLA with two interfaces and redundant tunnels using BGP dynamic routing. Classic VPN does not offer an SLA. Cloud Router handles BGP but is not a VPN itself."
    },
    {
      "domain_id": 3,
      "subtopic": "Deploying networking resources",
      "stem": "You want VMs without external IPs to access Google Cloud APIs. What should you enable on the subnet?",
      "choice_a": "Cloud NAT",
      "choice_b": "Private Google Access",
      "choice_c": "VPC Flow Logs",
      "choice_d": "Firewall logging",
      "correct_answer": "b",
      "explanation": "Private Google Access allows VMs with only internal IPs to reach Google APIs and services. Enable it per subnet. Cloud NAT is for general internet access, not specifically for Google APIs."
    },
    {
      "domain_id": 3,
      "subtopic": "Infrastructure as code",
      "stem": "You want to manage GCP infrastructure using declarative configuration files and track state. Which tool should you use?",
      "choice_a": "gcloud CLI scripts",
      "choice_b": "Terraform",
      "choice_c": "Cloud Shell",
      "choice_d": "Deployment Manager templates",
      "correct_answer": "b",
      "explanation": "Terraform uses declarative HCL configuration files and maintains a state file to track resources. It is the recommended IaC tool for GCP, supporting planning, applying, and destroying infrastructure."
    },
    {
      "domain_id": 3,
      "subtopic": "Infrastructure as code",
      "stem": "You are using Terraform with a team. Where should you store the Terraform state file?",
      "choice_a": "In the local filesystem",
      "choice_b": "In a Git repository",
      "choice_c": "In a Cloud Storage bucket with state locking",
      "choice_d": "In a Compute Engine VM",
      "correct_answer": "c",
      "explanation": "For team collaboration, store Terraform state in a Cloud Storage backend with state locking enabled. This prevents concurrent modifications and ensures everyone works with the same state."
    },
    {
      "domain_id": 3,
      "subtopic": "Infrastructure as code",
      "stem": "You want to manage GCP resources using Kubernetes-style YAML manifests. Which tool should you use?",
      "choice_a": "Terraform",
      "choice_b": "Config Connector",
      "choice_c": "Helm",
      "choice_d": "Cloud Foundation Toolkit",
      "correct_answer": "b",
      "explanation": "Config Connector is a Kubernetes add-on that maps GCP resources to Kubernetes custom resources. You manage GCP infrastructure using kubectl and YAML manifests, following the Kubernetes resource model."
    },
    {
      "domain_id": 3,
      "subtopic": "Infrastructure as code",
      "stem": "What does the 'terraform plan' command do?",
      "choice_a": "Creates all resources defined in the configuration",
      "choice_b": "Shows what changes will be made without applying them",
      "choice_c": "Initializes the Terraform working directory",
      "choice_d": "Destroys all managed resources",
      "correct_answer": "b",
      "explanation": "terraform plan creates an execution plan showing what Terraform will do (add, change, or destroy) to reach the desired state. It is a dry-run that makes no actual changes to infrastructure."
    },
    {
      "domain_id": 3,
      "subtopic": "Infrastructure as code",
      "stem": "You need to deploy a complex application with multiple microservices to GKE using pre-packaged templates. What tool should you use?",
      "choice_a": "Terraform",
      "choice_b": "Config Connector",
      "choice_c": "Helm",
      "choice_d": "Cloud Build",
      "correct_answer": "c",
      "explanation": "Helm is the Kubernetes package manager. It uses charts (pre-configured templates with values) to deploy complex applications. Charts can be customized via values files and stored in Artifact Registry."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Compute Engine resources",
      "stem": "You need to change a VM's machine type from n1-standard-4 to n1-standard-8. What must you do first?",
      "choice_a": "Create a snapshot of the VM",
      "choice_b": "Stop the VM",
      "choice_c": "Detach all disks",
      "choice_d": "Delete the VM and recreate it",
      "correct_answer": "b",
      "explanation": "To change a VM's machine type, you must first stop the VM. Then use gcloud compute instances set-machine-type to change the type, and start the VM again."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Compute Engine resources",
      "stem": "You want to create a custom image from an existing VM for use as a template. What is the recommended first step?",
      "choice_a": "Delete the VM",
      "choice_b": "Stop the VM",
      "choice_c": "Detach the boot disk",
      "choice_d": "Take a snapshot while the VM is running",
      "correct_answer": "b",
      "explanation": "Stop the VM first to ensure filesystem consistency. Then create an image from the stopped VM's disk using gcloud compute images create. Creating images from running VMs may result in inconsistent data."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Compute Engine resources",
      "stem": "Your team needs automated, scheduled snapshots of Compute Engine persistent disks. What should you use?",
      "choice_a": "A Cloud Scheduler job calling gcloud",
      "choice_b": "A snapshot schedule (resource policy)",
      "choice_c": "A Cloud Function triggered hourly",
      "choice_d": "Manual snapshots via a cron job on the VM",
      "correct_answer": "b",
      "explanation": "Snapshot schedules (resource policies) are the native GCP feature for automated periodic snapshots. They support hourly, daily, or weekly schedules and automatic retention management."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Compute Engine resources",
      "stem": "You cannot SSH into a VM. How can you troubleshoot the issue?",
      "choice_a": "Delete and recreate the VM",
      "choice_b": "Use the serial console to view boot logs",
      "choice_c": "Restart the VM from the Console",
      "choice_d": "Change the machine type",
      "correct_answer": "b",
      "explanation": "The serial console (gcloud compute connect-to-serial-port) provides access to the VM's serial output for debugging when SSH is unavailable. Enable serial-port-enable=TRUE in metadata first."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Compute Engine resources",
      "stem": "You need to increase the size of a 100 GB persistent disk to 500 GB without downtime. Is this possible?",
      "choice_a": "No, you must delete and recreate the disk",
      "choice_b": "Yes, but only after stopping the VM",
      "choice_c": "Yes, you can resize a persistent disk while it is attached to a running VM",
      "choice_d": "No, you must create a snapshot and create a new larger disk",
      "correct_answer": "c",
      "explanation": "Persistent disks can be resized (increased only) while attached to a running VM using gcloud compute disks resize. You must then also resize the filesystem from within the VM."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing GKE resources",
      "stem": "You need to add GPU nodes to your existing GKE cluster. What should you do?",
      "choice_a": "Modify the existing node pool to add GPUs",
      "choice_b": "Create a new node pool with GPU machine types",
      "choice_c": "Upgrade the cluster to enable GPU support",
      "choice_d": "Recreate the cluster with GPU nodes",
      "correct_answer": "b",
      "explanation": "You cannot modify an existing node pool's machine type. Create a new node pool with GPU-enabled machine types (e.g., n1-standard-4 with --accelerator type=nvidia-tesla-t4,count=1)."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing GKE resources",
      "stem": "You want to ensure your GKE cluster automatically adds or removes nodes based on pod resource demands. What should you enable?",
      "choice_a": "Horizontal Pod Autoscaler",
      "choice_b": "Cluster autoscaler",
      "choice_c": "Vertical Pod Autoscaler",
      "choice_d": "Node auto-provisioning",
      "correct_answer": "b",
      "explanation": "Cluster autoscaler adjusts the number of nodes in a node pool based on pending pods. If pods cannot be scheduled due to insufficient resources, it adds nodes. If nodes are underutilized, it removes them."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing GKE resources",
      "stem": "You need to update a deployment to a new container image without downtime. Which kubectl command should you use?",
      "choice_a": "kubectl delete deployment NAME && kubectl create deployment NAME --image=NEW_IMAGE",
      "choice_b": "kubectl set image deployment/NAME CONTAINER=NEW_IMAGE",
      "choice_c": "kubectl restart deployment NAME",
      "choice_d": "kubectl patch deployment NAME --type=replace",
      "correct_answer": "b",
      "explanation": "kubectl set image performs a rolling update, gradually replacing old pods with new ones. This ensures zero downtime. Deleting and recreating would cause downtime."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing GKE resources",
      "stem": "A node in your GKE cluster needs maintenance. How do you safely evict all pods from it?",
      "choice_a": "kubectl delete node NODE_NAME",
      "choice_b": "kubectl drain NODE_NAME --ignore-daemonsets --delete-emptydir-data",
      "choice_c": "kubectl cordon NODE_NAME",
      "choice_d": "gcloud container clusters resize --num-nodes=0",
      "correct_answer": "b",
      "explanation": "kubectl drain gracefully evicts all pods from a node and marks it as unschedulable. --ignore-daemonsets skips DaemonSet pods. kubectl cordon only prevents new scheduling but doesn't evict existing pods."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing GKE resources",
      "stem": "Where should you store Docker images that will be deployed to GKE, following current best practices?",
      "choice_a": "Docker Hub",
      "choice_b": "Cloud Storage",
      "choice_c": "Artifact Registry",
      "choice_d": "Container Registry",
      "correct_answer": "c",
      "explanation": "Artifact Registry is the recommended container image registry for GCP. It supports fine-grained IAM, multiple artifact formats, regional and multi-regional repositories. Container Registry is deprecated."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Cloud Run resources",
      "stem": "You deployed a new revision of your Cloud Run service and want to gradually roll it out by sending 10% of traffic to the new revision. How do you do this?",
      "choice_a": "gcloud run services update-traffic SERVICE --to-revisions=NEW_REV=10",
      "choice_b": "gcloud run deploy SERVICE --traffic=10",
      "choice_c": "kubectl set traffic --percent=10",
      "choice_d": "gcloud run services update SERVICE --split-traffic=10",
      "correct_answer": "a",
      "explanation": "gcloud run services update-traffic with --to-revisions allows you to specify traffic percentages per revision. The remaining traffic (90%) stays on the current serving revision."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Cloud Run resources",
      "stem": "Your Cloud Run service is experiencing slow responses during traffic spikes due to cold starts. What is the most effective solution?",
      "choice_a": "Increase --max-instances",
      "choice_b": "Increase --concurrency",
      "choice_c": "Set --min-instances greater than 0",
      "choice_d": "Increase --memory allocation",
      "correct_answer": "c",
      "explanation": "Setting --min-instances > 0 keeps warm instances always running, eliminating cold starts. This is the most direct solution for cold start issues, though it increases cost."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Cloud Run resources",
      "stem": "You need to roll back your Cloud Run service to a previous revision. What command should you use?",
      "choice_a": "gcloud run services rollback SERVICE",
      "choice_b": "gcloud run deploy SERVICE --image=OLD_IMAGE",
      "choice_c": "gcloud run services update-traffic SERVICE --to-revisions=OLD_REVISION=100",
      "choice_d": "gcloud run revisions restore OLD_REVISION",
      "correct_answer": "c",
      "explanation": "Route 100% of traffic to the previous revision using update-traffic. Cloud Run keeps old revisions available. There is no dedicated rollback command; you manage traffic routing instead."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Cloud Run resources",
      "stem": "Your Cloud Run service processes long-running requests (up to 10 minutes). What should you configure?",
      "choice_a": "--concurrency=1",
      "choice_b": "--timeout=600",
      "choice_c": "--max-instances=100",
      "choice_d": "--cpu-boost",
      "correct_answer": "b",
      "explanation": "Set --timeout=600 (600 seconds = 10 minutes) to allow requests up to 10 minutes. The default timeout is 300 seconds (5 minutes). Maximum is 3600 seconds (60 minutes)."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing Cloud Run resources",
      "stem": "You want your Cloud Run service to handle one request at a time per instance. What should you set?",
      "choice_a": "--min-instances=1",
      "choice_b": "--max-instances=1",
      "choice_c": "--concurrency=1",
      "choice_d": "--cpu=1",
      "correct_answer": "c",
      "explanation": "Setting --concurrency=1 ensures each container instance processes only one request at a time. New requests trigger new instances. Default concurrency is 80."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing storage and database solutions",
      "stem": "You want to automatically delete objects in a Cloud Storage bucket after 90 days. What should you configure?",
      "choice_a": "A Cloud Scheduler job",
      "choice_b": "An Object Lifecycle Management rule",
      "choice_c": "A Cloud Function triggered by object creation",
      "choice_d": "A retention policy",
      "correct_answer": "b",
      "explanation": "Object Lifecycle Management rules can automatically delete objects based on age, creation date, or other conditions. Configure a rule with action 'Delete' and condition 'age: 90'."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing storage and database solutions",
      "stem": "You need to move Cloud Storage objects from Standard to Nearline class after 30 days to reduce costs. What should you use?",
      "choice_a": "A gsutil command in a cron job",
      "choice_b": "Object Lifecycle Management with SetStorageClass action",
      "choice_c": "Transfer Service",
      "choice_d": "Autoclass",
      "correct_answer": "b",
      "explanation": "Object Lifecycle Management with the SetStorageClass action automatically transitions objects to Nearline (or other classes) based on age. This is the native, automated approach for storage class migration."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing storage and database solutions",
      "stem": "You accidentally deleted rows from a Cloud SQL database. The deletion happened 2 hours ago. How can you recover the data?",
      "choice_a": "Use point-in-time recovery to restore to a time before the deletion",
      "choice_b": "Restore from the latest automated backup",
      "choice_c": "Use the Cloud SQL undo feature",
      "choice_d": "Contact Google Support to recover the data",
      "correct_answer": "a",
      "explanation": "Point-in-time recovery (PITR) allows you to restore a Cloud SQL instance to any point in time within the binary log retention period. This is more precise than restoring from a backup."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing storage and database solutions",
      "stem": "You want to export BigQuery query results to Cloud Storage as CSV files. Which command should you use?",
      "choice_a": "bq extract --destination_format=CSV dataset.table gs://bucket/file.csv",
      "choice_b": "gcloud bigquery export dataset.table --format=csv --output=gs://bucket/file.csv",
      "choice_c": "bq cp dataset.table gs://bucket/file.csv",
      "choice_d": "gsutil cp bq://dataset.table gs://bucket/file.csv",
      "correct_answer": "a",
      "explanation": "bq extract exports a BigQuery table to Cloud Storage. Use --destination_format=CSV for CSV output. You can also export as JSON or Avro format."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing storage and database solutions",
      "stem": "You need a read-only copy of your Cloud SQL database in another region for disaster recovery. What should you create?",
      "choice_a": "A cross-region backup",
      "choice_b": "A cross-region read replica",
      "choice_c": "A Cloud SQL export to Cloud Storage",
      "choice_d": "A failover replica",
      "correct_answer": "b",
      "explanation": "A cross-region read replica provides a read-only copy of your database in another region. It can be promoted to a standalone instance for disaster recovery."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing networking resources",
      "stem": "VMs in your private subnet need to download packages from the internet but should not be directly accessible from outside. What should you set up?",
      "choice_a": "An external load balancer",
      "choice_b": "Cloud NAT",
      "choice_c": "A firewall rule allowing all ingress",
      "choice_d": "VPC Network Peering",
      "correct_answer": "b",
      "explanation": "Cloud NAT enables VMs without external IPs to access the internet for outbound connections (like downloading packages) without exposing them to inbound internet traffic."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing networking resources",
      "stem": "You need to register a DNS name and point it to your load balancer's IP. Which GCP service should you use?",
      "choice_a": "Cloud Armor",
      "choice_b": "Cloud DNS",
      "choice_c": "Cloud CDN",
      "choice_d": "Cloud Domains",
      "correct_answer": "b",
      "explanation": "Cloud DNS is GCP's managed DNS service. Create a managed zone for your domain and add an A record pointing to your load balancer's IP address."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing networking resources",
      "stem": "You need to expand a subnet from /24 to /20. Is this possible without recreating the subnet?",
      "choice_a": "No, subnets cannot be resized",
      "choice_b": "Yes, use gcloud compute networks subnets expand-ip-range",
      "choice_c": "Yes, but only by deleting and recreating with the new range",
      "choice_d": "Yes, but only if the subnet has no VMs",
      "correct_answer": "b",
      "explanation": "You can expand a subnet's IP range (decrease the prefix length) in place using gcloud compute networks subnets expand-ip-range. You cannot shrink a subnet. No downtime is required."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing networking resources",
      "stem": "You have ephemeral external IPs on your VMs that keep changing. How do you make them persistent?",
      "choice_a": "Use Cloud DNS with dynamic updates",
      "choice_b": "Promote the ephemeral IP to a static external IP",
      "choice_c": "Assign an internal IP instead",
      "choice_d": "Use a load balancer with a static IP",
      "correct_answer": "b",
      "explanation": "You can promote an existing ephemeral IP to static with gcloud compute addresses create NAME --addresses=EPHEMERAL_IP --region=REGION. The IP stays the same and persists through VM stops/starts."
    },
    {
      "domain_id": 4,
      "subtopic": "Managing networking resources",
      "stem": "You need internal DNS resolution within your VPC for a custom domain name. What should you create?",
      "choice_a": "A public Cloud DNS managed zone",
      "choice_b": "A private Cloud DNS managed zone",
      "choice_c": "A Cloud NAT gateway",
      "choice_d": "A DNS forwarding rule",
      "correct_answer": "b",
      "explanation": "A private Cloud DNS managed zone provides internal DNS resolution visible only to specified VPC networks. This is ideal for internal service discovery within your VPC."
    },
    {
      "domain_id": 4,
      "subtopic": "Monitoring and logging",
      "stem": "You need to be notified when CPU utilization on your VMs exceeds 85% for more than 5 minutes. What should you create?",
      "choice_a": "A log-based metric",
      "choice_b": "A Cloud Monitoring alerting policy",
      "choice_c": "A Cloud Function that checks CPU usage",
      "choice_d": "An exclusion filter",
      "correct_answer": "b",
      "explanation": "Create an alerting policy in Cloud Monitoring with a condition on the CPU utilization metric, a threshold of 0.85, and a duration of 5 minutes. Add a notification channel for alerts."
    },
    {
      "domain_id": 4,
      "subtopic": "Monitoring and logging",
      "stem": "You want to export all audit logs to a Cloud Storage bucket for long-term compliance. What should you create?",
      "choice_a": "A log-based metric",
      "choice_b": "A log sink with a Cloud Storage destination",
      "choice_c": "A Cloud Monitoring dashboard",
      "choice_d": "A BigQuery scheduled query",
      "correct_answer": "b",
      "explanation": "Create a log sink with a Cloud Storage destination and an appropriate filter (e.g., logName:\"cloudaudit.googleapis.com\"). The sink exports matching logs to the bucket for long-term storage."
    },
    {
      "domain_id": 4,
      "subtopic": "Monitoring and logging",
      "stem": "You want to reduce Cloud Logging costs by excluding verbose debug logs from ingestion. What should you configure?",
      "choice_a": "Delete the logs after ingestion",
      "choice_b": "An exclusion filter on the _Default sink",
      "choice_c": "A log-based metric to count debug logs",
      "choice_d": "A Cloud Monitoring alert for high log volume",
      "correct_answer": "b",
      "explanation": "Exclusion filters on the _Default log sink prevent matching log entries from being ingested into Cloud Logging, reducing costs. Excluded logs are not stored and cannot be queried."
    },
    {
      "domain_id": 4,
      "subtopic": "Monitoring and logging",
      "stem": "Which type of audit log records all API calls that modify resources (create, update, delete)?",
      "choice_a": "Data Access audit logs",
      "choice_b": "Admin Activity audit logs",
      "choice_c": "System Event audit logs",
      "choice_d": "Policy Denied audit logs",
      "correct_answer": "b",
      "explanation": "Admin Activity audit logs record API calls that modify resource configuration or metadata (create, update, delete). They are always enabled, cannot be disabled, and are free of charge."
    },
    {
      "domain_id": 4,
      "subtopic": "Monitoring and logging",
      "stem": "You need to collect system-level metrics (CPU, memory, disk) and custom application logs from Compute Engine VMs. What should you install?",
      "choice_a": "Cloud Monitoring agent (legacy)",
      "choice_b": "Cloud Logging agent (legacy)",
      "choice_c": "Ops Agent",
      "choice_d": "Prometheus exporter",
      "correct_answer": "c",
      "explanation": "The Ops Agent is the unified agent that replaces the legacy monitoring and logging agents. It collects both system metrics and logs, sending them to Cloud Monitoring and Cloud Logging."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing IAM",
      "stem": "A developer needs to view Compute Engine instances but not modify them. Which role should you assign?",
      "choice_a": "roles/viewer",
      "choice_b": "roles/compute.viewer",
      "choice_c": "roles/compute.admin",
      "choice_d": "roles/editor",
      "correct_answer": "b",
      "explanation": "roles/compute.viewer is a predefined role that grants read-only access to Compute Engine resources. roles/viewer (basic role) grants broad read access to all resources, violating least privilege."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing IAM",
      "stem": "You need to create a role that has only the permissions to start and stop VMs. What type of role should you create?",
      "choice_a": "A basic role",
      "choice_b": "A predefined role",
      "choice_c": "A custom role",
      "choice_d": "A primitive role",
      "correct_answer": "c",
      "explanation": "Custom roles allow you to define exactly which permissions to include. Create a custom role with compute.instances.start and compute.instances.stop permissions for this specific need."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing IAM",
      "stem": "A user was granted Editor role at the project level but you need to prevent them from deleting Cloud Storage buckets. What is the best approach?",
      "choice_a": "Remove the Editor role and grant specific roles instead",
      "choice_b": "Create an IAM deny policy that denies storage.buckets.delete",
      "choice_c": "Create an Organization Policy",
      "choice_d": "Set a bucket-level IAM policy",
      "correct_answer": "b",
      "explanation": "IAM deny policies explicitly block specific permissions and take precedence over allow policies. This is the most direct way to prevent a specific action while keeping the Editor role. Alternatively, replacing Editor with specific roles follows least privilege."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing IAM",
      "stem": "You want to grant a role that is only effective during business hours (9 AM - 5 PM). What IAM feature should you use?",
      "choice_a": "IAM deny policies",
      "choice_b": "IAM conditions",
      "choice_c": "Organization policies",
      "choice_d": "Custom roles",
      "correct_answer": "b",
      "explanation": "IAM conditions allow you to add time-based restrictions to role bindings. You can use the request.time attribute to restrict access to specific hours, making the role effective only during business hours."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing IAM",
      "stem": "A user reports they cannot access a BigQuery dataset. How can you diagnose the issue?",
      "choice_a": "Check the VPC firewall rules",
      "choice_b": "Use the IAM Policy Troubleshooter",
      "choice_c": "Check Cloud DNS settings",
      "choice_d": "Review Cloud Audit Logs for network errors",
      "correct_answer": "b",
      "explanation": "The IAM Policy Troubleshooter evaluates all allow and deny policies to determine why a principal does or does not have access to a resource. It shows which policies grant or deny the requested permission."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing service accounts",
      "stem": "Your application running on a Compute Engine VM needs to access Cloud Storage. What is the recommended way to authenticate?",
      "choice_a": "Store a service account key file on the VM",
      "choice_b": "Use the VM's attached service account with appropriate IAM roles",
      "choice_c": "Use the default Compute Engine service account with Editor role",
      "choice_d": "Embed API credentials in the application code",
      "correct_answer": "b",
      "explanation": "Attach a custom service account with minimal required roles to the VM. The application automatically receives credentials via the metadata server. This avoids managing key files."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing service accounts",
      "stem": "A developer needs to test with a service account's permissions without downloading a key. What should they use?",
      "choice_a": "Run the application on a VM with the service account attached",
      "choice_b": "Use service account impersonation with --impersonate-service-account",
      "choice_c": "Create a temporary service account key",
      "choice_d": "Grant the service account's roles to the developer",
      "correct_answer": "b",
      "explanation": "Service account impersonation (gcloud --impersonate-service-account=SA_EMAIL) lets users act as a SA without keys. Requires roles/iam.serviceAccountTokenCreator on the target SA."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing service accounts",
      "stem": "Your GKE pods need to access Google Cloud services. What is the recommended authentication method?",
      "choice_a": "Mount a service account key as a Kubernetes secret",
      "choice_b": "Use Workload Identity",
      "choice_c": "Use the node's service account for all pods",
      "choice_d": "Store service account keys in environment variables",
      "correct_answer": "b",
      "explanation": "Workload Identity links Kubernetes service accounts to Google Cloud service accounts. Pods automatically get credentials for the mapped GSA without needing keys. This is the recommended approach for GKE."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing service accounts",
      "stem": "You want to temporarily disable a service account that may be compromised, without losing its configuration. What should you do?",
      "choice_a": "Delete the service account",
      "choice_b": "Remove all IAM bindings from the service account",
      "choice_c": "Disable the service account",
      "choice_d": "Rotate all service account keys",
      "correct_answer": "c",
      "explanation": "Disabling a service account (gcloud iam service-accounts disable SA_EMAIL) prevents it from authenticating while preserving its configuration and IAM bindings. Re-enable later with the enable command."
    },
    {
      "domain_id": 5,
      "subtopic": "Managing service accounts",
      "stem": "What is the security concern with the default Compute Engine service account?",
      "choice_a": "It has no permissions by default",
      "choice_b": "It is granted the Editor role by default, which is overly broad",
      "choice_c": "It cannot be used with IAM",
      "choice_d": "It expires after 24 hours",
      "correct_answer": "b",
      "explanation": "The default Compute Engine service account (PROJECT_NUMBER-compute@developer.gserviceaccount.com) is granted the Editor role by default, which provides broad access. Best practice is to use a custom SA with minimal permissions."
    }
  ]
}
